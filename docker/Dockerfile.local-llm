# ABOUTME: Local LLM container running Ollama with native OpenAI API support
# - Runs Ollama with Gemma model
# - Exposes OpenAI-compatible API on port 11434

FROM python:3.13-slim

ARG DEBIAN_FRONTEND=noninteractive

# System deps
RUN apt-get update && apt-get install -y \
    curl \
    ca-certificates && \
    rm -rf /var/lib/apt/lists/*

# Install Ollama
RUN curl -fsSL https://ollama.com/install.sh | sh

# Runtime env
ENV OLLAMA_HOST=0.0.0.0
EXPOSE 11434

# Entrypoint to run Ollama
COPY docker/local-llm-entrypoint.sh /usr/local/bin/local-llm-entrypoint.sh
RUN chmod +x /usr/local/bin/local-llm-entrypoint.sh

HEALTHCHECK --interval=10s --timeout=5s --retries=5 CMD curl -f http://localhost:11434/api/tags || exit 1

CMD ["/usr/local/bin/local-llm-entrypoint.sh"]
