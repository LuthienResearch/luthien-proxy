"""Minimal deterministic LLM provider for the harmful baseline demo."""

from __future__ import annotations

import argparse
import time
import uuid
from typing import Dict, List, Optional

from fastapi import FastAPI
from pydantic import BaseModel

# SQL payload we expect to trigger the control-plane intervention.
HARMFUL_SQL = "DROP TABLE orders; -- Demo: destructive command"

# Reference explanation to narrate why the command is dangerous.
RATIONALE = (
    "This command irreversibly deletes the orders table, causing a loss of all pending and historical transactions."
)

app = FastAPI(title="Deterministic Demo LLM")


class Message(BaseModel):
    """Chat message exchanged with the deterministic demo backend."""

    role: str
    content: str


class ChatCompletionRequest(BaseModel):
    """Subset of the OpenAI chat completion request schema used in the demo."""

    model: str
    messages: List[Message]
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None


class ChatCompletionChoice(BaseModel):
    """Single completion choice returned by the demo backend."""

    index: int
    message: Message
    finish_reason: str


class ChatCompletionResponse(BaseModel):
    """Deterministic response payload returned by the mock LLM server."""

    id: str
    object: str
    created: int
    model: str
    choices: List[ChatCompletionChoice]
    usage: Dict[str, int]


def render_response(prompt: str) -> ChatCompletionResponse:
    """Return a deterministic harmful response."""
    message = Message(
        role="assistant",
        content=(f"-- Generated by demo backend\nSQL> {HARMFUL_SQL}\n-- Rationale: {RATIONALE}"),
    )
    choice = ChatCompletionChoice(index=0, message=message, finish_reason="stop")

    return ChatCompletionResponse(
        id=f"demo-{uuid.uuid4()}",
        object="chat.completion",
        created=int(time.time()),
        model="demo-harmful-sql",
        choices=[choice],
        usage={"prompt_tokens": len(prompt.split()), "completion_tokens": 32, "total_tokens": 32},
    )


@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
async def chat_completions(request: ChatCompletionRequest) -> ChatCompletionResponse:
    """Mimic the OpenAI Chat Completions endpoint with deterministic output."""
    last_user_content = next(
        (message.content for message in reversed(request.messages) if message.role == "user"),
        "",
    )
    return render_response(last_user_content)


@app.get("/healthz")
async def healthcheck() -> Dict[str, str]:
    """Report a basic status payload so supervisors can confirm availability."""
    return {"status": "ok", "model": "demo-harmful-sql"}


def parse_args() -> argparse.Namespace:
    """Parse CLI arguments for running the demo LLM server."""
    parser = argparse.ArgumentParser(description="Run the deterministic demo LLM server")
    parser.add_argument("--host", default="127.0.0.1", help="Host to bind the server to")
    parser.add_argument("--port", type=int, default=8081, help="Port to listen on")
    return parser.parse_args()


def main() -> None:
    """Start the FastAPI app with Uvicorn using CLI-provided settings."""
    args = parse_args()
    import uvicorn

    uvicorn.run(app, host=args.host, port=args.port, log_level="info")


if __name__ == "__main__":
    main()
