# V2 Observability Demonstration Guide

This guide walks through testing the UppercaseNthWordPolicy and exploring all V2 observability features.

## Prerequisites

1. **Start the observability stack**:
   ```bash
   ./scripts/observability.sh start
   ```
   This starts Tempo, Loki, Promtail, and Grafana.

2. **Verify services are running**:
   ```bash
   docker compose ps
   ```
   Should show: control-plane, postgres, redis, tempo, loki, promtail, grafana all running.

3. **Set up environment** (if not already done):
   ```bash
   cp .env.example .env
   # Edit .env to set your API keys if needed
   ```

## Part 1: Start the V2 Gateway

The V2 gateway is now configured with `UppercaseNthWordPolicy(n=3)` which will uppercase every 3rd word in responses.

```bash
# Start the V2 gateway (in a separate terminal)
uv run python -m luthien_proxy.v2.main
```

You should see logs indicating:
- `UppercaseNthWordPolicy initialized with n=3`
- `Control plane initialized with OpenTelemetry tracing`
- `Application startup complete`

The gateway is now running at `http://localhost:8000`.

## Part 2: Send Test Requests

### Option A: Using curl (Non-streaming)

```bash
curl -s "http://localhost:8000/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "model": "gpt-3.5-turbo",
    "messages": [{"role": "user", "content": "Tell me a short story about a cat"}],
    "max_tokens": 100,
    "stream": false
  }' | jq
```

**Expected**: Every 3rd word in the response should be UPPERCASE.

Example output:
```
Original: "Once upon a time there was a cat named Whiskers..."
Transformed: "Once upon A time there WAS a cat NAMED Whiskers..."
```

### Option B: Using curl (Streaming)

```bash
curl -N "http://localhost:8000/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "model": "gpt-3.5-turbo",
    "messages": [{"role": "user", "content": "Count from one to ten"}],
    "max_tokens": 50,
    "stream": true
  }'
```

Watch the words stream out with every 3rd word uppercased in real-time!

### Option C: Using Claude Code through the proxy

This is the most interesting option! Configure Claude Code to use the proxy:

1. **Find your Claude Code settings** (typically `~/.claude/config.json` or similar)

2. **Add proxy configuration**:
   ```json
   {
     "api": {
       "baseUrl": "http://localhost:8000/v1",
       "apiKey": "YOUR_API_KEY"
     }
   }
   ```

3. **Start a Claude Code session** and ask it something like:
   - "Explain how binary search works"
   - "Write a function to reverse a string"
   - "Tell me about quantum computing"

Watch as every 3rd word gets uppercased!

## Part 3: Real-Time Monitoring with Activity Monitor

While requests are flowing, open the **Activity Monitor**:

```bash
open http://localhost:8000/v2/activity/monitor
```

### What You'll See:

1. **Real-time event stream** - Events appear as they happen:
   - `gateway.request_received` - Blue border
   - `gateway.request_sent` - Purple border
   - `gateway.response_received` - Green border
   - `gateway.response_sent` - Teal border
   - `policy.uppercase_*` - Orange border (policy events)

2. **Filter the events**:
   - **By Call ID**: Copy a call_id from an event, paste into filter â†’ see all events for that request
   - **By Model**: Type "gpt-3.5" â†’ see only GPT-3.5 requests
   - **By Event Type**: Select "Policy Events" â†’ see only policy transformations

3. **Event Details** you can inspect:
   - Call ID (for correlation across tools)
   - Model being used
   - Request/response content (preview)
   - Policy event summaries (e.g., "Uppercased every 3th word in response")

### Try This:

1. Send several requests with different models
2. Use the model filter to show only specific models
3. Pick a call_id and copy it (we'll use it next!)

## Part 4: Deep Dive with Diff Viewer

Now let's see **exactly** what the policy changed. Open the **Diff Viewer**:

```bash
open http://localhost:8000/v2/debug/diff
```

### Viewing a Specific Call:

1. **Paste the call_id** you copied from the activity monitor
2. Click "Load Diff"

### What You'll See:

**Request Diff (Left/Right columns)**:
- Original messages on the left
- Final messages on the right (should be identical - policy doesn't change requests)
- Metadata changes highlighted (if any)

**Response Diff (Left/Right columns)**:
- **Original Content**: The LLM's raw response
- **Final Content**: After policy transformation (every 3rd word UPPERCASED!)
- Changed text is highlighted in orange
- Word-by-word comparison

**Grafana Trace Link**:
- Click the "ðŸ“Š View Trace in Grafana" button
- Opens Tempo with the full distributed trace

### Browse Recent Calls:

1. Click "Browse Recent" button
2. See the last 20 calls with timestamps
3. Click any call to load its diff

### JSON Highlighting:

If the response contains JSON (from a tool call or structured output), you'll see:
- Syntax highlighting (keys in blue, strings in green, numbers in orange)
- Pretty-printed formatting
- But the policy doesn't uppercase JSON - only text content!

## Part 5: Performance Analysis with Grafana

Open Grafana:

```bash
open http://localhost:3000
```

Navigate to **"V2 Metrics & Performance"** dashboard (should auto-provision).

### Panels to Explore:

**1. Request Rate by Model** (Top Left)
- Time series showing requests/second
- Grouped by model (gpt-3.5-turbo, gpt-4, claude, etc.)
- See traffic patterns in real-time

**2. Policy Execution Latency (p95)** (Top Right)
- 95th percentile latency for policy operations
- Separate lines for:
  - `control_plane.process_request` (request policies)
  - `control_plane.process_response` (response policies)
  - `control_plane.process_streaming_response` (streaming policies)
- **Question**: Is the uppercase policy adding latency? Check the graph!

**3. Total V2 Requests** (Middle Left)
- Gauge showing total request count
- Color changes: Green â†’ Yellow (50+) â†’ Red (100+)

**4. Avg Request Latency** (Middle Center)
- Average end-to-end latency
- Shows if system is getting slower

**5. Recent Traces** (Middle Bottom Left)
- Table of last 20 traces
- **Call ID column** - Copy this to use in diff viewer!
- Click a row â†’ opens full trace view in Tempo

**6. Errors & Warnings** (Middle Bottom Right)
- Live log stream filtered to errors/warnings
- If policy fails, you'll see it here

**7. Latency Breakdown** (Bottom - Full Width)
- Stacked bar chart showing where time is spent:
  - Gateway overhead
  - Policy processing time
  - LLM response time
- **Question**: Is policy processing fast compared to LLM?

### Try This:

1. Send 10-20 requests in quick succession
2. Watch the request rate spike in the graph
3. Check if policy latency stays consistent
4. Click a trace, then use its call_id in the diff viewer
5. From the trace view, click "View Logs" â†’ see correlated logs in Loki

## Part 6: Trace Correlation (The Full Picture)

Let's trace a single request through the entire system:

1. **Send a request** (note the response)

2. **Activity Monitor** â†’ Find the request by model/time
   - Copy the call_id

3. **Diff Viewer** â†’ Paste call_id
   - See the before/after transformation
   - Click "View Trace in Grafana"

4. **Grafana Tempo** â†’ Examine the trace:
   - `gateway.chat_completions` span (root)
     - `control_plane.process_request` span
       - `policy.process_request` span
     - `litellm.completion` span (LLM call)
     - `control_plane.process_response` span
       - `policy.process_full_response` span

5. **From the trace** â†’ Click "Logs for this span"
   - See structured logs with full context
   - Filter by span_id or trace_id

6. **Back to Diff Viewer** â†’ Verify the transformation matches what you saw

## Part 7: Testing Edge Cases

### Very Short Response

```bash
curl -s "http://localhost:8000/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "model": "gpt-3.5-turbo",
    "messages": [{"role": "user", "content": "Say: one two three four five"}],
    "max_tokens": 20
  }' | jq
```

Expected: "one two THREE four FIVE" (or similar short response)

### Streaming with Word Boundaries

Send a streaming request and watch how the policy handles word boundaries across chunks.

### Multiple Requests in Parallel

Open multiple terminal windows and send requests simultaneously:

```bash
# Terminal 1
for i in {1..5}; do
  curl -s "http://localhost:8000/v1/chat/completions" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer YOUR_API_KEY" \
    -d "{\"model\":\"gpt-3.5-turbo\",\"messages\":[{\"role\":\"user\",\"content\":\"Count to $i\"}],\"max_tokens\":20}" &
done
wait
```

Then check:
- Activity Monitor â†’ See all 5 requests
- Grafana â†’ See the request rate spike
- Diff Viewer â†’ Browse recent calls to see all 5

## Part 8: Policy Events in Detail

Back to the **Activity Monitor**, filter to show only **Policy Events**:

1. Select "Policy Events" in the event type dropdown
2. Observe the policy event details:
   - `policy.uppercase_request` - "Request passed through (policy only affects responses, n=3)"
   - `policy.uppercase_applied` - "Uppercased every 3th word in response"
     - Details: original_preview, transformed_preview, word_count, n
   - `policy.uppercase_streaming_started` - "Started streaming transformation (n=3)"
   - `policy.uppercase_streaming_complete` - "Completed streaming transformation"
     - Details: chunks_processed, words_transformed, n

## Summary: What We've Demonstrated

âœ… **Real-time Monitoring**: Activity Monitor showing live events with filtering
âœ… **Policy Debugging**: Diff Viewer showing exact before/after changes
âœ… **Performance Metrics**: Grafana dashboard with latency, rates, and traces
âœ… **Distributed Tracing**: Tempo traces correlating requests across components
âœ… **Log Correlation**: Loki logs linked to traces via call_id
âœ… **Event Emission**: Policy emitting structured events for observability

## Key Observability Features

1. **Call ID Correlation**: Same call_id across all systems (activity monitor, diff viewer, Grafana, Tempo, Loki)
2. **Non-blocking Persistence**: Events written to PostgreSQL via background queue (no request latency impact)
3. **Real-time Streaming**: SSE-based activity monitor updates instantly
4. **Structured Diffs**: API computes diffs server-side for easy visualization
5. **Trace-to-Logs**: Click from trace spans directly to correlated log entries
6. **Policy Transparency**: Every policy transformation is visible and traceable

## Troubleshooting

**Activity Monitor not showing events?**
- Check Redis is running: `docker compose ps redis`
- Check V2 gateway logs for Redis connection errors

**Diff Viewer returns 404?**
- Check PostgreSQL is running: `docker compose ps postgres`
- Verify events were written: `uv run python scripts/query_debug_logs.py --call-id <call_id>`

**Grafana dashboard empty?**
- Check Tempo is receiving traces: `docker compose logs tempo`
- Verify OTEL_ENABLED=true in environment
- Check traces: `curl http://localhost:3200/api/search`

**Policy not transforming text?**
- Check V2 gateway logs for policy initialization
- Verify UppercaseNthWordPolicy is configured in main.py
- Test with non-streaming first (easier to debug)

## Next Steps

- Try changing `n` to a different value (e.g., n=2 for every other word, n=1 for all words)
- Create your own policy by copying UppercaseNthWordPolicy
- Add more complex transformations (e.g., word filtering, content moderation)
- Set up alerting rules in Grafana for policy failures or high latency
- Export interesting traces and diffs for documentation
