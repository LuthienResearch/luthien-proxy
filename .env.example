# Luthien Control Environment Configuration
# Copy this file to .env and fill in your actual values
#
# Quick start: Only ANTHROPIC_API_KEY is truly required.
# Everything else has working defaults for local development.

# =============================================================================
# REQUIRED: Your LLM Provider API Key(s)
# =============================================================================
# The proxy uses these to authenticate to upstream providers (Anthropic, OpenAI)

ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Optional: Only needed if routing to OpenAI models
# OPENAI_API_KEY=your_openai_api_key_here

# =============================================================================
# GATEWAY AUTHENTICATION (defaults work for local dev)
# =============================================================================

# PROXY_API_KEY: Clients use this to authenticate to the Luthien proxy
# Claude Code connects with: ANTHROPIC_API_KEY=sk-luthien-dev-key
PROXY_API_KEY=sk-luthien-dev-key

# ADMIN_API_KEY: For admin UI and policy management
# Change this in production!
ADMIN_API_KEY=admin-dev-key

# =============================================================================
# PORT CONFIGURATION (change if you have conflicts)
# =============================================================================

# Gateway port - this is where you point your client
# Change this if 8000 conflicts with another service
GATEWAY_PORT=8000
GATEWAY_HOST=localhost

# =============================================================================
# POLICY CONFIGURATION
# =============================================================================

# POLICY_SOURCE: How to load and persist policies
# Options:
#   - "db": Use database only (production, UI-managed)
#   - "file": Use YAML file only (local dev, CI/CD, read-only)
#   - "db-fallback-file": Try DB first, fall back to file (recommended for production)
#   - "file-fallback-db": Try file first, fall back to DB (migration scenario)
POLICY_SOURCE=db-fallback-file

# POLICY_CONFIG: Path to YAML policy configuration file
# Used when POLICY_SOURCE includes "file"
POLICY_CONFIG=/app/config/policy_config.yaml

# =============================================================================
# INFRASTRUCTURE (Docker services - usually leave as-is)
# =============================================================================

# PostgreSQL - stores conversation events and policy config
POSTGRES_USER=luthien
POSTGRES_PASSWORD=luthien_dev_password
POSTGRES_DB=luthien_control
POSTGRES_PORT=5432
DATABASE_URL=postgresql://luthien:luthien_dev_password@db:5432/luthien_control

# Redis Configuration
REDIS_URL=redis://redis:6379
REDIS_PORT=6379


# Local LLM Gateway (OpenAI-compatible) for policy scoring
LOCAL_LLM_PORT=4010

# Ollama (local model host) port
OLLAMA_PORT=11434


# Test model used by local scripts (trace/sanity/tests)
# Set this to a model that exists for your API keys when calling through the proxy
# e.g., gpt-4o, gpt-4o-mini, or a local model via the local LLM gateway
TEST_MODEL=gpt-4o-mini

# =============================================================================
# OBSERVABILITY (optional - for distributed tracing)
# =============================================================================

# OpenTelemetry Configuration
# Set to http://tempo:4317 when running with observability stack (./scripts/observability.sh up)
# Leave empty to disable tracing
OTEL_EXPORTER_OTLP_ENDPOINT=http://tempo:4317

# Toggle OpenTelemetry tracing (default: true)
OTEL_ENABLED=true

# Service metadata for distributed tracing
SERVICE_NAME=luthien-proxy
SERVICE_VERSION=2.0.0
ENVIRONMENT=development

# Grafana URL for viewing traces and logs
# Used by debug endpoints to generate links to Grafana Tempo traces
GRAFANA_URL=http://localhost:3000

# =============================================================================
# LLM JUDGE POLICIES (optional - for SimpleJudgePolicy)
# =============================================================================

# Used by ToolCallJudgePolicy and SimpleJudgePolicy
# By default, uses local Ollama. Override to use OpenAI/Anthropic as judge.
# LLM_JUDGE_MODEL=openai/gpt-4
# LLM_JUDGE_API_BASE=http://localhost:11434/v1
# LLM_JUDGE_API_KEY=your_judge_api_key

# =============================================================================
# ADVANCED (rarely needed)
# =============================================================================

# LiteLLM Configuration
# Master key for LiteLLM if using multi-tenant setup
# LITELLM_MASTER_KEY=your_litellm_master_key
