# Luthien Control Environment Configuration
# Copy this file to .env and fill in your actual values
#
# MULTI-DEPLOYMENT SUPPORT
# Ports are auto-selected at startup if not pinned here.
# Just run `./scripts/quick_start.sh` and it finds free ports automatically.
# To pin a specific port, uncomment the relevant line below.
#
# For multiple deployments on the same machine, each deployment needs a unique
# COMPOSE_PROJECT_NAME (to isolate Docker networks, volumes, and containers).

# Docker Compose project name â€” isolates Docker networks, volumes, and containers.
# Used by quick_start.sh, observability.sh, and direct `docker compose` calls.
# Override in your .env to avoid collisions between worktrees.
COMPOSE_PROJECT_NAME=luthien-proxy

# LLM Provider API Keys (used by the proxy to authenticate to upstream providers)
# These are the REAL API keys that the proxy uses to call OpenAI/Anthropic
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Gateway Configuration
# PROXY_API_KEY: The API key that clients use to authenticate to the Luthien proxy
# Claude Code and other clients will use this key to authenticate to the proxy
PROXY_API_KEY=sk-luthien-dev-key

# ADMIN_API_KEY: The API key for admin/policy management operations
# Required for accessing /api/admin/* endpoints (policy configuration UI)
# Set this to a secure value in production
ADMIN_API_KEY=admin-dev-key

# AUTH_MODE: How clients authenticate to the proxy
# Options:
#   - "proxy_key": Only accept PROXY_API_KEY (strict, for controlled environments)
#   - "passthrough": Accept any valid Anthropic API key (validated via count_tokens)
#   - "both": Accept proxy key OR valid Anthropic key (default, recommended)
# Default is "both" so Claude Code works whether using API keys or Claude Max (OAuth/relay).
# AUTH_MODE=both

GATEWAY_HOST=localhost
# Uncomment to pin a specific port (otherwise auto-selected at startup)
# GATEWAY_PORT=8000

# Policy Configuration
# POLICY_SOURCE: How to load and persist policies
# Options:
#   - "db": Use database only (production, UI-managed)
#   - "file": Use YAML file only (local dev, CI/CD, read-only)
#   - "db-fallback-file": Try DB first, fall back to file (recommended for production)
#   - "file-fallback-db": Try file first, fall back to DB (migration scenario)
POLICY_SOURCE=db-fallback-file

# POLICY_CONFIG: Path to YAML policy configuration file
# Used when POLICY_SOURCE includes "file"
POLICY_CONFIG=/app/config/policy_config.yaml

# DOGFOOD_MODE: Auto-compose DogfoodSafetyPolicy around any configured policy.
# Blocks dangerous commands (docker compose down, etc.) that could kill the proxy
# when an AI agent is running through it. Uses fast regex matching, zero latency.
# DOGFOOD_MODE=false

# PostgreSQL Configuration
POSTGRES_USER=luthien
POSTGRES_PASSWORD=luthien_dev_password
POSTGRES_DB=luthien_control
# Uncomment to pin a specific port (otherwise auto-selected at startup)
# POSTGRES_PORT=5433
DATABASE_URL=postgresql://luthien:luthien_dev_password@db:5432/luthien_control

# Redis Configuration
REDIS_URL=redis://redis:6379
# Uncomment to pin a specific port (otherwise auto-selected at startup)
# REDIS_PORT=6379


# Test model used by local scripts (trace/sanity/tests)
# Set this to a model that exists for your API keys when calling through the proxy
TEST_MODEL=gpt-4o-mini

# OpenTelemetry Configuration
# Set to http://tempo:4317 when running with observability stack (./scripts/observability.sh up)
# Leave empty to disable tracing
OTEL_EXPORTER_OTLP_ENDPOINT=http://tempo:4317

# Toggle OpenTelemetry tracing (default: true)
OTEL_ENABLED=true

# Service metadata for distributed tracing
SERVICE_NAME=luthien-proxy
SERVICE_VERSION=2.0.0
ENVIRONMENT=development

# Observability Ports (only used with --profile observability)
# Uncomment to pin specific ports (otherwise auto-selected at startup)
# TEMPO_OTLP_PORT=4317
# TEMPO_HTTP_PORT=3200

# LLM Judge Configuration (optional, for judge-based policies)
# Used by ToolCallJudgePolicy and SimpleJudgePolicy
# LLM_JUDGE_MODEL=openai/gpt-4
# LLM_JUDGE_API_BASE=http://localhost:11434/v1
# LLM_JUDGE_API_KEY=your_judge_api_key

# LiteLLM Configuration (optional)
# Master key for LiteLLM if using multi-tenant setup
# LITELLM_MASTER_KEY=your_litellm_master_key
