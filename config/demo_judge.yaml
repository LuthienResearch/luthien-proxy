# Demo configuration with LLM Judge policy (AI control active)
# Used to show what happens WITH AI Control

policy:
  class: "luthien_proxy.policies.tool_call_judge:LLMJudgeToolPolicy"
  config:
    model: "ollama/gemma2:2b"
    api_base: "http://local-llm:11434"
    api_key: "ollama"
    probability_threshold: 0.6
    temperature: 0.0
    max_tokens: 256
